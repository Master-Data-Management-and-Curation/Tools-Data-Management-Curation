{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 file from NANOPORE SEQUENCING SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsson_filename = 'L4085/report_PAG26975_20240513_1631_a46c69aa.json'\n",
    "html_filename = 'L4085/report_PAG26975_20240513_1631_a46c69aa.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadati json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsson_filename) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = data['protocol_run_info']['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasformo in dict la lista data['protocol_run_info']['args']\n",
    "\n",
    "def convert_value(value):\n",
    "    # Prova a convertire in int\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Prova a convertire in float\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Ritorna la stringa così com'è se non è int o float\n",
    "    return value\n",
    "\n",
    "args_dict = {}\n",
    "\n",
    "for i in range(len(args_list)):\n",
    "    item = args_list[i]\n",
    "    if '=' in item:\n",
    "        key, value = item.lstrip('--').split('=', 1)\n",
    "        args_dict[key] = convert_value(value)\n",
    "    elif item.startswith('--'):\n",
    "        key = item.lstrip('--')\n",
    "        value = args_list[i + 1] if i + 1 < len(args_list) and not args_list[i + 1].startswith('--') else None\n",
    "        args_dict[key] = convert_value(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['protocol_run_info']['args'] = args_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasformo in dict la lista data['protocol_run_info']['device']['firmware_version']\n",
    "# ogni elemento della lista diventa un nuovo dizionario firmware_i\n",
    "\n",
    "firmware_dict = {f'firmware_{i+1}': firmware for i, firmware in enumerate(data['protocol_run_info']['device']['firmware_version'])}\n",
    "data['protocol_run_info']['device']['firmware_version'] = firmware_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimino un layer di dict in data['protocol_run_info']['meta_info']['tags'] mantenendo i tipi appropriati\n",
    "transformed_dict = {}\n",
    "\n",
    "for key, sub_dict in data['protocol_run_info']['meta_info']['tags'].items():\n",
    "    if 'string_value' in sub_dict:\n",
    "        transformed_dict[key] = sub_dict['string_value']\n",
    "    elif 'bool_value' in sub_dict:\n",
    "        transformed_dict[key] = sub_dict['bool_value']\n",
    "    elif 'array_value' in sub_dict:\n",
    "        transformed_dict[key] = ast.literal_eval(sub_dict['array_value'])  # Converts the string representation of a list into an actual list\n",
    "\n",
    "data['protocol_run_info']['meta_info']['tags'] = transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasformo in dict la lista data['acquisitions']\n",
    "# ogni elemento della lista diventa un nuovo dizionario acquisition_i\n",
    "\n",
    "acquisition_dict = {f'acquisition_{i+1}': firmware for i, firmware in enumerate(data['acquisitions'])}\n",
    "data['acquisitions'] = acquisition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ciascuna acquisition_i trasformo la lista \n",
    "# data['acquisitions']['acquisition_i']['acquisition_run_info']['config_summary']['channel_state_info']['groups']\n",
    "# in un dizionario\n",
    "\n",
    "for acquisition_key in data['acquisitions']:\n",
    "    groups = data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups']\n",
    "    transformed_groups = {d['name']: {k: v for k, v in d.items() if k != 'name'} for d in groups}\n",
    "    data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups'] = transformed_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ciascuna acquisition_i trasformo la lista \n",
    "# data['acquisitions']['acquisition_i']['bias_voltage']\n",
    "# in un dizionario\n",
    "\n",
    "for acquisition_key in data['acquisitions']:\n",
    "        bias_voltage = data['acquisitions'][acquisition_key]['bias_voltage']\n",
    "        transformed_bias_voltage = {f'bias_voltage_{i+1}': bias_voltage[i] for i in range(len(bias_voltage))}\n",
    "        for bv_key in transformed_bias_voltage:\n",
    "            tmp = transformed_bias_voltage[bv_key]['bias_voltages']\n",
    "            transformed_bias_voltage2 = {f'info_{k+1}': tmp[k] for k in range(len(tmp))}\n",
    "            transformed_bias_voltage[bv_key]['bias_voltages'] = transformed_bias_voltage2\n",
    "        data['acquisitions'][acquisition_key]['bias_voltage'] = transformed_bias_voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ciascuna acquisition_i trasformo la lista \n",
    "# data['acquisitions']['acquisition_1']['temperature']\n",
    "# in un dizionario\n",
    "\n",
    "for temperature_key in data['acquisitions']:\n",
    "        temperature = data['acquisitions'][temperature_key]['temperature']\n",
    "        transformed_temperature = {f'temperature_{i+1}': temperature[i] for i in range(len(temperature))}\n",
    "        for child_key in transformed_temperature:\n",
    "            tmp = transformed_temperature[child_key]['temperatures']\n",
    "            transformed_temperature2 = {f'info_{k+1}': tmp[k] for k in range(len(tmp))}\n",
    "            transformed_temperature[child_key]['temperatures'] = transformed_temperature2\n",
    "        data['acquisitions'][temperature_key]['temperature'] = transformed_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sequencing', 'pore', 'recovering', 'inactive', 'unclassified'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per ciascuna acquisition_i trasformo la lista \n",
    "# data['acquisitions']['acquisition_i']['acquisition_run_info']['config_summary']['channel_state_info']['groups'][group_key]['states']\n",
    "# in un dizionario\n",
    "\n",
    "for acquisition_key in data['acquisitions']:\n",
    "    for group_key in data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups']:\n",
    "        states = data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups'][group_key]['states']\n",
    "        transformed_states = {d['name']: {k: v for k, v in d.items() if k != 'name'} for d in states}\n",
    "        data['acquisitions'][acquisition_key]['acquisition_run_info']['config_summary']['channel_state_info']['groups'][group_key]['states'] = transformed_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_hdf5(d, hdf5_group, path=''):\n",
    "    \"\"\"\n",
    "    Recursively converts a nested dictionary to an HDF5 file structure.\n",
    "\n",
    "    Args:\n",
    "        d (dict): The dictionary to iterate through.\n",
    "        hdf5_group (h5py.Group): The current HDF5 group to write to.\n",
    "        path (str): The current path in the HDF5 file (for naming purposes).\n",
    "    \"\"\"\n",
    "    #excluded_keys = [\"acquisitions\",'user_messages','report_data_generation_time']\n",
    "    excluded_keys = ['user_messages',\n",
    "                     'report_data_generation_time',\n",
    "                     'duty_time',\n",
    "                     'writer_output',\n",
    "                     'acquisition_output',\n",
    "                     'read_length_histogram',\n",
    "                     'basecall_boxplot',\n",
    "                     'qscore_histograms'\n",
    "                     ]\n",
    "    for key, value in d.items():\n",
    "        if key in excluded_keys:\n",
    "            # Skip this key and its children\n",
    "            continue\n",
    "\n",
    "        # Determine if the key name already exists and adjust if necessary\n",
    "        group_name = key\n",
    "        count = 1\n",
    "        while group_name in hdf5_group:\n",
    "            count += 1\n",
    "            group_name = f\"{key}_{count}\"\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            # Create a new group for this key\n",
    "            new_group = hdf5_group.create_group(group_name)\n",
    "            # Recurse into the dictionary\n",
    "            dict_to_hdf5(value, new_group, path=f\"{path}/{group_name}\")\n",
    "        else:\n",
    "            # Handle non-dict values by checking their type\n",
    "            if isinstance(value, str):\n",
    "                # Convert a single string to variable-length strings\n",
    "                dt = h5py.string_dtype(encoding='utf-8')\n",
    "                hdf5_group.create_dataset(group_name, data=value, dtype=dt)\n",
    "            elif isinstance(value, (list, tuple)):\n",
    "                if all(isinstance(item, str) for item in value):\n",
    "                    # Convert list of strings to numpy array with appropriate dtype\n",
    "                    dt = h5py.string_dtype(encoding='utf-8')\n",
    "                    hdf5_group.create_dataset(group_name, data=np.array(value, dtype=dt), dtype=dt)\n",
    "                elif all(isinstance(item, (int, float)) for item in value):\n",
    "                    # Convert list of numbers to numpy array\n",
    "                    hdf5_group.create_dataset(group_name, data=np.array(value))\n",
    "                else:\n",
    "                    # Convert mixed-type lists to a JSON string and store it\n",
    "                    json_data = json.dumps(value)\n",
    "                    dt = h5py.string_dtype(encoding='utf-8')\n",
    "                    hdf5_group.create_dataset(group_name, data=json_data, dtype=dt)\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                if value.dtype == object:\n",
    "                    # Convert numpy arrays with dtype=object to JSON string and store it\n",
    "                    json_data = json.dumps(value.tolist())\n",
    "                    dt = h5py.string_dtype(encoding='utf-8')\n",
    "                    hdf5_group.create_dataset(group_name, data=json_data, dtype=dt)\n",
    "                else:\n",
    "                    hdf5_group.create_dataset(group_name, data=value)\n",
    "            elif isinstance(value, (int, float, np.number)):\n",
    "                # Directly store numerical values\n",
    "                hdf5_group.create_dataset(group_name, data=value)\n",
    "            else:\n",
    "                # If the value is not a supported type, raise an error\n",
    "                raise TypeError(f\"Unsupported data type for key '{group_name}': {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the HDF5 file\n",
    "with h5py.File('report.hdf5', 'w') as hdf5_file:\n",
    "    dict_to_hdf5(data, hdf5_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica e analizza l'HTML\n",
    "with open(html_filename, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Utilizza BeautifulSoup per analizzare l'HTML\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Trova il tag script che contiene `reportData`\n",
    "script_tag = soup.find('script', text=lambda text: text and 'reportData=' in text)\n",
    "\n",
    "if script_tag:\n",
    "    # Estrai il testo del tag script\n",
    "    script_content = script_tag.string\n",
    "\n",
    "    # Trova il punto in cui inizia `reportData`\n",
    "    start_index = script_content.find('reportData=') + len('reportData=')\n",
    "    \n",
    "    # Trova la fine dell'oggetto JSON\n",
    "    end_index = script_content.find(';', start_index)\n",
    "    \n",
    "    # Estrai la stringa JSON\n",
    "    json_data = script_content[start_index:end_index].strip()\n",
    "    \n",
    "    # Converti la stringa JSON in un oggetto Python\n",
    "    report_data = json.loads(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# riapro il file in rw\n",
    "f = h5py.File('report.hdf5', \"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo gruppo summary\n",
    "f.create_group(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in report_data['run_setup']:\n",
    "    print(i['title'], i['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo e popolo sottogruppi\n",
    "f['/summary'].create_group(\"run_setup\")\n",
    "for i in report_data['run_setup']:\n",
    "    f['/summary/run_setup'].create_dataset(i['title'], data=i['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/summary'].create_group(\"run_settings\")\n",
    "for i in report_data['run_settings']:\n",
    "    f['/summary/run_settings'].create_dataset(i['title'], data=i['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/summary'].create_group(\"data_output_settings\")\n",
    "for i in report_data['data_output_settings']:\n",
    "    f['/summary/data_output_settings'].create_dataset(i['title'], data=i['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/summary'].create_group(\"software_versions\")\n",
    "for i in report_data['software_versions']:\n",
    "    f['/summary/software_versions'].create_dataset(i['title'], data=i['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/summary'].create_group(\"data_output\")\n",
    "for key, value in report_data['data_output'].items():\n",
    "    f['/summary/data_output'].create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/summary'].create_group(\"basecalling\")\n",
    "for key, value  in report_data['basecalling'].items():\n",
    "    f['/summary/basecalling'].create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ LENGTHS · OUTLIERS REMOVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_lengths = data['acquisitions']['acquisition_4']['read_length_histogram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_lengths[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5 = r_lengths[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(int, test_5['plot']['histogram_data'][0]['bucket_values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0,len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'istogramma\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('Read length (k)')\n",
    "plt.ylabel('Bases (Mb)')\n",
    "plt.title('READ LENGTHS · OUTLIERS REMOVED')\n",
    "#plt.xticks(rotation=90)  # Ruota le etichette per renderle più leggibili\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUMULATIVE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_0 = data['acquisitions']['acquisition_4']['acquisition_output'][0]['plot'][0]['snapshots'][0]['snapshots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [int(i[\"seconds\"]) for i in test_0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds_to_hhmm(seconds_list):\n",
    "    hhmm_list = []\n",
    "    for seconds in seconds_list:\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        hhmm_list.append(f\"{int(hours):02d}:{int(minutes):02d}\")\n",
    "    return hhmm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time = convert_seconds_to_hhmm(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_passed = [int(i[\"yield_summary\"][\"basecalled_pass_bases\"]) for i in test_0]\n",
    "bases_failed = [int(i[\"yield_summary\"][\"basecalled_fail_bases\"]) for i in test_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_passed_Gb = [i*1e-9 for i in bases_passed]\n",
    "bases_failed_Gb = [i*1e-9 for i in bases_failed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_total_Gb = [x + y for x, y in zip(bases_passed_Gb, bases_failed_Gb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'istogramma\n",
    "plt.figure(figsize=(30, 8))\n",
    "plt.plot(new_time, bases_passed_Gb)\n",
    "plt.plot(new_time, bases_failed_Gb)\n",
    "plt.plot(new_time, bases_total_Gb)\n",
    "plt.xlabel('Time (Hrs:Min)')\n",
    "plt.ylabel('Bases (Gb)')\n",
    "plt.title('CUMULATIVE OUTPUT: BASES')\n",
    "#plt.xticks(rotation=90)  # Ruota le etichette per renderle più leggibili\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUALITY SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qs = data['acquisitions']['acquisition_4']['qscore_histograms'][0]['histogram_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qs[0]['filtering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qs[1]['filtering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_qs_passed = [int(x) for x in test_qs[0]['bucket_values']]\n",
    "reads_qs_failed = [int(x) for x in test_qs[1]['bucket_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qs = list(range(0,len(reads_qs_passed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'istogramma\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(x_qs, reads_qs_passed)\n",
    "plt.bar(x_qs, reads_qs_failed)\n",
    "plt.xlabel('Q Score')\n",
    "plt.ylabel('Reads')\n",
    "plt.title('QUALITY SCORE')\n",
    "#plt.xticks(rotation=90)  # Ruota le etichette per renderle più leggibili\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# riapro il file in rw\n",
    "f = h5py.File('report.hdf5', \"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_group(\"plots\")\n",
    "f['/plots'].attrs[\"NX_class\"] = \"NXentry\"\n",
    "f['/plots'].attrs[\"default\"] = \"cumulative_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative output\n",
    "f['/plots'].create_group(\"cumulative_output\")\n",
    "f['/plots/cumulative_output'].attrs[\"NX_class\"] = \"NXdata\"\n",
    "f['/plots/cumulative_output'].attrs[\"signal\"] = \"Bases_passed\"\n",
    "f['/plots/cumulative_output'].attrs[\"auxiliary_signals\"] = ['Bases_total','Bases_failed']\n",
    "f['/plots/cumulative_output'].attrs[\"axes\"] = 'Time'\n",
    "f['/plots/cumulative_output'].create_dataset('title',data='Cumulative Output')\n",
    "\n",
    "f['/plots/cumulative_output'].create_dataset('Time', data=time)\n",
    "f['/plots/cumulative_output'].create_dataset('Bases_passed', data=bases_passed_Gb)\n",
    "f['/plots/cumulative_output'].create_dataset('Bases_failed', data=bases_failed_Gb)\n",
    "f['/plots/cumulative_output'].create_dataset('Bases_total', data=bases_total_Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lengths\n",
    "f['/plots'].create_group(\"read_lengths\")\n",
    "f['/plots/read_lengths'].attrs[\"NX_class\"] = \"NXdata\"\n",
    "f['/plots/read_lengths'].attrs[\"signal\"] = \"Bases\"\n",
    "f['/plots/read_lengths'].attrs[\"axes\"] = 'Read_length'\n",
    "f['/plots/read_lengths'].create_dataset('title',data='Read Lengths')\n",
    "\n",
    "f['/plots/read_lengths'].create_dataset('Bases', data=y)\n",
    "f['/plots/read_lengths'].create_dataset('Read_length', data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality score\n",
    "f['/plots'].create_group(\"q_score\")\n",
    "f['/plots/q_score'].attrs[\"NX_class\"] = \"NXdata\"\n",
    "f['/plots/q_score'].attrs[\"signal\"] = \"Read_passed\"\n",
    "f['/plots/q_score'].attrs[\"auxiliary_signals\"] = ['Read_failed']\n",
    "f['/plots/q_score'].attrs[\"axes\"] = 'Q_Score'\n",
    "f['/plots/q_score'].create_dataset('title',data='Quality Score')\n",
    "\n",
    "f['/plots/q_score'].create_dataset('Q_Score', data=x_qs)\n",
    "f['/plots/q_score'].create_dataset('Read_passed', data=reads_qs_passed)\n",
    "f['/plots/q_score'].create_dataset('Read_failed', data=reads_qs_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
